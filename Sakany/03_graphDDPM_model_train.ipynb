{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for graph nodes features generation - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch_geometric\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import pyvista as pv\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose backend for matplotlib\n",
    "# -----------------------------\n",
    "from IPython import get_ipython\n",
    "# get_ipython().run_line_magic('matplotlib', 'widget')\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# Or simply:\n",
    "# %matplotlib widget\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose backend for pyvista with jupyter\n",
    "# ---------------------------------------\n",
    "# pv.set_jupyter_backend('trame')  # 3D-interactive plots\n",
    "pv.set_jupyter_backend('static') # static plots\n",
    "\n",
    "# Notes:\n",
    "# -> ignored if run in a standard python shell\n",
    "# -> use keyword argument \"notebook=False\" in Plotter() to open figure in a pop-up window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load local functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load local functions...')\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(1, '../utils/')\n",
    "\n",
    "# from graph_utils import *\n",
    "# from graph_ddpm import *\n",
    "# from ml_utils import *\n",
    "# from graph_plot import *\n",
    "# # from magic_utils import *\n",
    " \n",
    "with open('../utils/graph_utils.py') as f: exec(f.read())\n",
    "with open('../utils/graph_ddpm.py') as f: exec(f.read())\n",
    "with open('../utils/ml_utils.py') as f: exec(f.read())\n",
    "with open('../utils/graph_plot.py') as f: exec(f.read())\n",
    "# with open('../utils/magic_utils.py') as f: exec(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameters\n",
    "\n",
    "Some parameters (dimension / attribute considered and indexes / parameters for plotting graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load parameters...')\n",
    "\n",
    "# from params import *\n",
    "\n",
    "with open('params.py') as f: exec(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output settings\n",
    "For saving data set and model (once trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Define output settings...')\n",
    "\n",
    "# Output directory (for saving)\n",
    "# -----------------------------\n",
    "out_dir = 'out_graphDDPM_model' # output directory\n",
    "\n",
    "fig_dir = 'fig'      # PARAMS\n",
    "\n",
    "plt_show = True      # PARAMS (show graphics 2D ?)\n",
    "off_screen = False   # PARAMS (show graphics 3D ?)\n",
    "\n",
    "save_fig_png = True  # PARAMS\n",
    "fig_prefix = '03'    # PARAMS\n",
    "\n",
    "fig_counter = 0\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "if not os.path.isdir(fig_dir):\n",
    "    os.mkdir(fig_dir)\n",
    "\n",
    "# Files for saving data set / test set (pickle) (see further)\n",
    "# -----------------------------------------------------------\n",
    "filename_data_set = os.path.join(out_dir, f'data_set.pickle')\n",
    "filename_data_set_shift = os.path.join(out_dir, f'data_set_shift.txt')\n",
    "filename_data_set_scale_factor = os.path.join(out_dir, f'data_set_scale_factor.txt')\n",
    "filename_test_set = os.path.join(out_dir, f'test_set.pickle')\n",
    "\n",
    "# Files for saving network (ddpm) (see further)\n",
    "# ---------------------------------------------\n",
    "filename_hyper_param_ddpm_net = os.path.join(out_dir, 'ddpm_net_hyper_params.txt')\n",
    "filename_hyper_param_ddpm     = os.path.join(out_dir, 'ddpm_hyper_params.txt')\n",
    "\n",
    "filename_param_ddpm     = os.path.join(out_dir, 'ddpm.params')\n",
    "\n",
    "# Files for saving loss and lr (see further)\n",
    "# -------------------------------------------\n",
    "filename_loss_lr = os.path.join(out_dir, 'ddpm_loss_lr.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set / test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read graphs collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Read data set / test set (collection of subgraphs)...')\n",
    "\n",
    "# Load from pickle file\n",
    "data_dir = 'data_gen'\n",
    "filename_graph_collection_data_set = os.path.join(data_dir, f'graph_collection_data_set.pickle')\n",
    "filename_graph_collection_test_set = os.path.join(data_dir, f'graph_collection_test_set.pickle')\n",
    "\n",
    "with open(filename_graph_collection_data_set, 'rb') as f: G_list = pickle.load(f)\n",
    "with open(filename_graph_collection_test_set, 'rb') as f: G_list_test = pickle.load(f)\n",
    "\n",
    "# Set parameters\n",
    "if attr is not None:\n",
    "    node_attrs      = ['pos', attr]\n",
    "    node_attrs_ind  = [tuple(range(dim)), [dim + i for i in range(attr_ncomp)]]\n",
    "    node_attrs_type = ['float', 'float']\n",
    "else:\n",
    "    node_attrs      = ['pos']\n",
    "    node_attrs_ind  = [tuple(range(dim))]\n",
    "    node_attrs_type = ['float']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert graphs from networkx to torch_geometric and rescale features\n",
    "Each feature (coordinate of position or attribute) should be \"stationary\", i.e. similar statistics on every (sub)graph of the data set / test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert graphs from networkx to torch_geometric\n",
    "Set ensemble of node features of interest in new features \"x\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Convert graph from data set / test set from networkx to torch_geometric...')\n",
    "\n",
    "G_geom_list      = [torch_geometric.utils.from_networkx(G, group_node_attrs=node_attrs) for G in G_list]\n",
    "G_geom_list_test = [torch_geometric.utils.from_networkx(G, group_node_attrs=node_attrs) for G in G_list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of graphs in list for data set: {len(G_geom_list)}')\n",
    "print(f'Number of graphs in list for test set: {len(G_geom_list_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute \"shift\" and \"scaling factor\" for rescaling - on data set\n",
    "\n",
    "Basic statistics of node features on data set: mean and variance will be used for \"shift\" and \"scaling\", see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Compute statistics (marginal) on nodes features of graphs in data set...')\n",
    "\n",
    "# Node features on data set\n",
    "node_features = [G_geom.x for G_geom in G_geom_list]\n",
    "\n",
    "# Stats for every graph separately\n",
    "node_features_min  = [x.min(axis=0).values  for x in node_features]\n",
    "node_features_max  = [x.max(axis=0).values  for x in node_features]\n",
    "node_features_mean = [x.mean(axis=0) for x in node_features]\n",
    "node_features_var  = [x.var(axis=0)  for x in node_features]\n",
    "\n",
    "# Mean of statistics over every graph\n",
    "mean_of_min  = torch.vstack(node_features_min).mean(dim=0) \n",
    "mean_of_max  = torch.vstack(node_features_max).mean(dim=0) \n",
    "mean_of_mean = torch.vstack(node_features_mean).mean(dim=0) \n",
    "mean_of_var  = torch.vstack(node_features_var).mean(dim=0)\n",
    "\n",
    "# Variance of mean\n",
    "var_of_mean = torch.vstack(node_features_mean).var(dim=0)\n",
    "\n",
    "# # --- or mean weighted by the number of nodes... ---\n",
    "# n_nodes  = [x.shape[0] for x in node_features]\n",
    "# weight = torch.tensor(n_nodes).view(-1, 1).repeat(1, 5)/torch.tensor(n_nodes).sum() # weight of each graph (repeated on each row)\n",
    "# mean_of_min  = (torch.vstack(node_features_min) * weight).sum(dim=0)\n",
    "# mean_of_max  = (torch.vstack(node_features_max) * weight).sum(dim=0)\n",
    "# mean_of_mean = (torch.vstack(node_features_mean) * weight).sum(dim=0)\n",
    "# mean_of_var  = (torch.vstack(node_features_var) * weight).sum(dim=0)\n",
    "# var_of_mean = ((torch.vstack(node_features_mean)-mean_of_mean)**2 * weight).sum(dim=0)\n",
    "# # ---\n",
    "\n",
    "# Global variance\n",
    "var_all = var_of_mean + mean_of_var\n",
    "\n",
    "# Stats for all graphs together\n",
    "all_graph_node_features_min  = torch.vstack(node_features).min(dim=0).values\n",
    "all_graph_node_features_max  = torch.vstack(node_features).max(dim=0).values\n",
    "all_graph_node_features_mean = torch.vstack(node_features).mean(dim=0)\n",
    "all_graph_node_features_var  = torch.vstack(node_features).var(dim=0)\n",
    "all_graph_node_features_std  = torch.vstack(node_features).std(dim=0)\n",
    "\n",
    "print('Mean of stats on node features for every graph separately')\n",
    "print('mean_of_min  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_min]))\n",
    "print('mean_of_max  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_max]))\n",
    "print('mean_of_mean = ' + ' '.join([f'{x:15.9f}' for x in mean_of_mean]))\n",
    "print('mean_of_var  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_var]))\n",
    "print('')\n",
    "print('Variance of stats on node features for every graph separately')\n",
    "print('var_of_mean  = ' + ' '.join([f'{x:15.9f}' for x in var_of_mean]))\n",
    "print('')\n",
    "print('Stats on node features for all graphs together')\n",
    "print('min_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_min]))\n",
    "print('max_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_max]))\n",
    "print('mean_all     = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_mean]))\n",
    "print('var_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_var]))\n",
    "print('std_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_std]))\n",
    "print('')\n",
    "print('Global variance = mean of variance + variance of mean')\n",
    "print('                    [internal var. + between variance]')\n",
    "print('mean_of_var + var_of_mean')\n",
    "print('             = ' + ' '.join([f'{x:15.9f}' for x in var_all]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set \"shift\" and \"scaling factor\" for rescaling - from data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Set \"shift\" and \"scaling factor\" from data set...')\n",
    "\n",
    "# # Based on mean of statistics for every graph separately\n",
    "# # ------------------------------------------------------\n",
    "# # Shift from original features to new ones (from data set)\n",
    "# node_features_shift = - mean_of_mean \n",
    "\n",
    "# # Scaling factor from original features to new ones (from data set)\n",
    "# node_features_scale_factor = 1.0 / torch.sqrt(mean_of_var)\n",
    "\n",
    "# Based on statistics for all graphs together\n",
    "# -------------------------------------------\n",
    "# Shift from original features to new ones (from data set)\n",
    "node_features_shift = - all_graph_node_features_mean\n",
    "\n",
    "# Scaling factor from original features to new ones (from data set)\n",
    "node_features_scale_factor = 1.0 / all_graph_node_features_std\n",
    "# node_features_scale_factor = 1.0 / torch.sqrt(all_graph_node_features_var)\n",
    "\n",
    "# Notes: \n",
    "# -----\n",
    "#    mean_of_mean = all_graph_node_features_mean\n",
    "#    mean_of_var = all_graph_node_features_var - var_of_mean <= all_graph_node_features_var\n",
    "\n",
    "print(f'\"shift\"         : node_features_shift        = {\", \".join([f\"{x:.5g}\" for x in node_features_shift])}')\n",
    "print(f'\"scaling factor\": node_features_scale_factor = {\", \".join([f\"{x:.5g}\" for x in node_features_scale_factor])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center and rescale node features (data set / test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rescale (center and rescale) data set and test set...')\n",
    "\n",
    "# Center and rescale each features of all graphs with same factor\n",
    "# - data set\n",
    "for G_geom in G_geom_list:\n",
    "    G_geom.x = node_features_scale_factor * (G_geom.x + node_features_shift)\n",
    "\n",
    "# - test set\n",
    "for G_geom in G_geom_list_test:\n",
    "    G_geom.x = node_features_scale_factor * (G_geom.x + node_features_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics of node features on rescaled data set and rescaled test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Compute statistics (marginal) on nodes features of graphs in \"rescaled\" data set...')\n",
    "\n",
    "# Node features on data set\n",
    "node_features = [G_geom.x for G_geom in G_geom_list]\n",
    "\n",
    "# Stats for every graph separately\n",
    "node_features_min  = [x.min(axis=0).values  for x in node_features]\n",
    "node_features_max  = [x.max(axis=0).values  for x in node_features]\n",
    "node_features_mean = [x.mean(axis=0) for x in node_features]\n",
    "node_features_var  = [x.var(axis=0)  for x in node_features]\n",
    "\n",
    "# Mean of statistics over every graph\n",
    "mean_of_min  = torch.vstack(node_features_min).mean(dim=0) \n",
    "mean_of_max  = torch.vstack(node_features_max).mean(dim=0) \n",
    "mean_of_mean = torch.vstack(node_features_mean).mean(dim=0) \n",
    "mean_of_var  = torch.vstack(node_features_var).mean(dim=0)\n",
    "\n",
    "# Variance of mean\n",
    "var_of_mean = torch.vstack(node_features_mean).var(dim=0)\n",
    "\n",
    "# # --- or mean weighted by the number of nodes... ---\n",
    "# n_nodes  = [x.shape[0] for x in node_features]\n",
    "# weight = torch.tensor(n_nodes).view(-1, 1).repeat(1, 5)/torch.tensor(n_nodes).sum() # weight of each graph (repeated on each row)\n",
    "# mean_of_min  = (torch.vstack(node_features_min) * weight).sum(dim=0)\n",
    "# mean_of_max  = (torch.vstack(node_features_max) * weight).sum(dim=0)\n",
    "# mean_of_mean = (torch.vstack(node_features_mean) * weight).sum(dim=0)\n",
    "# mean_of_var  = (torch.vstack(node_features_var) * weight).sum(dim=0)\n",
    "# var_of_mean = ((torch.vstack(node_features_mean)-mean_of_mean)**2 * weight).sum(dim=0)\n",
    "# # ---\n",
    "\n",
    "# Global variance\n",
    "var_all = var_of_mean + mean_of_var\n",
    "\n",
    "# Stats for all graphs together\n",
    "all_graph_node_features_min  = torch.vstack(node_features).min(dim=0).values\n",
    "all_graph_node_features_max  = torch.vstack(node_features).max(dim=0).values\n",
    "all_graph_node_features_mean = torch.vstack(node_features).mean(dim=0)\n",
    "all_graph_node_features_var  = torch.vstack(node_features).var(dim=0)\n",
    "all_graph_node_features_std  = torch.vstack(node_features).std(dim=0)\n",
    "\n",
    "print('Mean of stats on node features for every graph separately')\n",
    "print('mean_of_min  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_min]))\n",
    "print('mean_of_max  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_max]))\n",
    "print('mean_of_mean = ' + ' '.join([f'{x:15.9f}' for x in mean_of_mean]))\n",
    "print('mean_of_var  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_var]))\n",
    "print('')\n",
    "print('Variance of stats on node features for every graph separately')\n",
    "print('var_of_mean  = ' + ' '.join([f'{x:15.9f}' for x in var_of_mean]))\n",
    "print('')\n",
    "print('Stats on node features for all graphs together')\n",
    "print('min_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_min]))\n",
    "print('max_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_max]))\n",
    "print('mean_all     = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_mean]))\n",
    "print('var_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_var]))\n",
    "print('std_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_std]))\n",
    "print('')\n",
    "print('Global variance = mean of variance + variance of mean')\n",
    "print('                    [internal var. + between variance]')\n",
    "print('mean_of_var + var_of_mean')\n",
    "print('             = ' + ' '.join([f'{x:15.9f}' for x in var_all]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Compute statistics (marginal) on nodes features of graphs in \"rescaled\" test set...')\n",
    "\n",
    "# Node features on test set\n",
    "node_features = [G_geom.x for G_geom in G_geom_list_test]\n",
    "\n",
    "# Stats for every graph separately\n",
    "node_features_min  = [x.min(axis=0).values  for x in node_features]\n",
    "node_features_max  = [x.max(axis=0).values  for x in node_features]\n",
    "node_features_mean = [x.mean(axis=0) for x in node_features]\n",
    "node_features_var  = [x.var(axis=0)  for x in node_features]\n",
    "\n",
    "# Mean of statistics over every graph\n",
    "mean_of_min  = torch.vstack(node_features_min).mean(dim=0) \n",
    "mean_of_max  = torch.vstack(node_features_max).mean(dim=0) \n",
    "mean_of_mean = torch.vstack(node_features_mean).mean(dim=0) \n",
    "mean_of_var  = torch.vstack(node_features_var).mean(dim=0)\n",
    "\n",
    "# Variance of mean\n",
    "var_of_mean = torch.vstack(node_features_mean).var(dim=0)\n",
    "\n",
    "# # --- or mean weighted by the number of nodes... ---\n",
    "# n_nodes  = [x.shape[0] for x in node_features]\n",
    "# weight = torch.tensor(n_nodes).view(-1, 1).repeat(1, 5)/torch.tensor(n_nodes).sum() # weight of each graph (repeated on each row)\n",
    "# mean_of_min  = (torch.vstack(node_features_min) * weight).sum(dim=0)\n",
    "# mean_of_max  = (torch.vstack(node_features_max) * weight).sum(dim=0)\n",
    "# mean_of_mean = (torch.vstack(node_features_mean) * weight).sum(dim=0)\n",
    "# mean_of_var  = (torch.vstack(node_features_var) * weight).sum(dim=0)\n",
    "# var_of_mean = ((torch.vstack(node_features_mean)-mean_of_mean)**2 * weight).sum(dim=0)\n",
    "# # ---\n",
    "\n",
    "# Global variance\n",
    "var_all = var_of_mean + mean_of_var\n",
    "\n",
    "# Stats for all graphs together\n",
    "all_graph_node_features_min  = torch.vstack(node_features).min(dim=0).values\n",
    "all_graph_node_features_max  = torch.vstack(node_features).max(dim=0).values\n",
    "all_graph_node_features_mean = torch.vstack(node_features).mean(dim=0)\n",
    "all_graph_node_features_var  = torch.vstack(node_features).var(dim=0)\n",
    "all_graph_node_features_std  = torch.vstack(node_features).std(dim=0)\n",
    "\n",
    "print('Mean of stats on node features for every graph separately')\n",
    "print('mean_of_min  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_min]))\n",
    "print('mean_of_max  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_max]))\n",
    "print('mean_of_mean = ' + ' '.join([f'{x:15.9f}' for x in mean_of_mean]))\n",
    "print('mean_of_var  = ' + ' '.join([f'{x:15.9f}' for x in mean_of_var]))\n",
    "print('')\n",
    "print('Variance of stats on node features for every graph separately')\n",
    "print('var_of_mean  = ' + ' '.join([f'{x:15.9f}' for x in var_of_mean]))\n",
    "print('')\n",
    "print('Stats on node features for all graphs together')\n",
    "print('min_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_min]))\n",
    "print('max_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_max]))\n",
    "print('mean_all     = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_mean]))\n",
    "print('var_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_var]))\n",
    "print('std_all      = ' + ' '.join([f'{x:15.9f}' for x in all_graph_node_features_std]))\n",
    "print('')\n",
    "print('Global variance = mean of variance + variance of mean')\n",
    "print('                    [internal var. + between variance]')\n",
    "print('mean_of_var + var_of_mean')\n",
    "print('             = ' + ' '.join([f'{x:15.9f}' for x in var_all]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data set (rescaled) and the test set (rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Define the data set / test set for graphDDPM...')\n",
    "\n",
    "# Data set\n",
    "G_nsample = np.full((len(G_geom_list), ), 1) # number of times each graph in the list is sampled\n",
    "data_set = Graph_geom_sampler_data_set(G_geom_list, G_nsample)\n",
    "\n",
    "# Test set\n",
    "G_nsample_test = np.full((len(G_geom_list_test), ), 1) # number of times each graph in the list is sampled\n",
    "test_set = Graph_geom_sampler_data_set(G_geom_list_test, G_nsample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: some graphs from the data set and test are plotted in next notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data via a data loader, and plot first graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Define data loader...')\n",
    "\n",
    "# Data loader (pytorch)\n",
    "# ---------------------\n",
    "batch_size = 6\n",
    "data_loader = torch_geometric.loader.DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "#data_loader = torch_geometric.loader.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot first batches (2D)...')\n",
    "\n",
    "torch.random.manual_seed(293) # -> for reproducibility of batches delivered by the data loader (if needed)\n",
    "\n",
    "# 2D view \n",
    "# =======\n",
    "kwds = kwds_multi.copy()\n",
    "\n",
    "figsize = figsize_lh3\n",
    "# -----\n",
    "\n",
    "same_color_bar = False\n",
    "\n",
    "for i_batch, G_batch in enumerate(data_loader):\n",
    "    if i_batch == 3:\n",
    "        break\n",
    "\n",
    "    G_batch_geom_list = G_batch.to_data_list()    \n",
    "    out_name = f'ddpm_train_set_2d_batch_{i_batch}'\n",
    "\n",
    "    plot_graph_multi_2d_from_G_geom_list(\n",
    "            G_batch_geom_list, dim,\n",
    "            out_name=out_name, \n",
    "            nr=None,\n",
    "            attr=attr,\n",
    "            attr_label_list=attr_label_list, \n",
    "            attr_cmap_list=attr_cmap_list,\n",
    "            rescale=False,\n",
    "            title_list=None, title_fontsize=12,\n",
    "            figsize=figsize, save_fig_png=save_fig_png, \n",
    "            filename_prefix=f'{fig_dir}/{fig_prefix}_{fig_counter:02d}',\n",
    "            with_labels=False, same_color_bar=same_color_bar, show_color_bar=True,\n",
    "            show=plt_show,\n",
    "            **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%skip_if dim == 2\n",
    "if dim == 3:\n",
    "    print('Plot first batches (3D)...')\n",
    "    \n",
    "    torch.random.manual_seed(293) # -> for reproducibility of batches delivered by the data loader (if needed)\n",
    "\n",
    "    # 3D view \n",
    "    # =======\n",
    "    kwargs_edges = kwargs_edges_multi.copy()\n",
    "    kwargs_pts = kwargs_pts_multi.copy()\n",
    "    kwargs_pts_labels = kwargs_pts_labels_multi.copy()\n",
    "    kwargs_scalar_bar = kwargs_scalar_bar_multi.copy()\n",
    "\n",
    "    window_size = [int(0.66*x) for x in window_size_multi]\n",
    "    # -----\n",
    "\n",
    "    notebook = True  # inline\n",
    "    cpos = None\n",
    "\n",
    "    same_color_bar = False\n",
    "\n",
    "    for i_batch, G_batch in enumerate(data_loader):\n",
    "        if i_batch == 3:\n",
    "            break\n",
    "\n",
    "        G_batch_geom_list = G_batch.to_data_list()\n",
    "        out_name = f'ddpm_train_set_3d_batch_{i_batch}'\n",
    "\n",
    "        plot_graph_multi_3d_from_G_geom_list(\n",
    "                G_batch_geom_list, dim,\n",
    "                out_name=out_name, \n",
    "                nr=None,\n",
    "                attr=attr,\n",
    "                attr_label_list=attr_label_list, \n",
    "                attr_cmap_list=attr_cmap_list,\n",
    "                rescale=False,\n",
    "                title_list=None, title_fontsize=12,\n",
    "                notebook=notebook, window_size=window_size, save_fig_png=save_fig_png, off_screen=off_screen,\n",
    "                filename_prefix=f'{fig_dir}/{fig_prefix}_{fig_counter:02d}',\n",
    "                with_labels=False, same_color_bar=same_color_bar, show_color_bar=True,\n",
    "                kwargs_edges=kwargs_edges, kwargs_pts=kwargs_pts, kwargs_scalar_bar=kwargs_scalar_bar, kwargs_pts_labels=kwargs_pts_labels,\n",
    "                cpos=cpos, print_cpos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig_png:\n",
    "    fig_counter = fig_counter+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data set into training set and validation set by random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Split the data set into training set and validation set (random sampling)...')\n",
    "\n",
    "# Split data set into training set and validation set\n",
    "seed = 234\n",
    "valid_frac = 0.2\n",
    "\n",
    "n = len(data_set.G_geom_list)\n",
    "G_geom_list = [data_set.G_geom_list[i] for i in torch.randperm(n)]\n",
    "\n",
    "n_valid = int(valid_frac*n)\n",
    "n_train = n - n_valid\n",
    "\n",
    "train_set = Graph_geom_sampler_data_set(G_geom_list[:n_train], np.full((n_train, ), 1))\n",
    "valid_set = Graph_geom_sampler_data_set(G_geom_list[n_train:], np.full((n_valid, ), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM model for graph node features generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model (design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Define the model (design)...')\n",
    "\n",
    "# Model\n",
    "# -----\n",
    "\n",
    "# Number of time steps\n",
    "# n_steps = 2000\n",
    "# n_steps = 2400\n",
    "n_steps = 2400\n",
    "\n",
    "# Embedding dimension for time steps\n",
    "time_emb_dim = 50 #200\n",
    "\n",
    "# Noise schedule\n",
    "\n",
    "# # - constant\n",
    "# betas = 1.e-3\n",
    "\n",
    "# # - linear\n",
    "# # betas = np.linspace(1.e-4, 2.e-2, n_steps).tolist()\n",
    "# betas = np.linspace(1.e-4, 1.e-2, n_steps).tolist()\n",
    "\n",
    "# # - cosine (https://arxiv.org/pdf/2102.09672.pdf)\n",
    "# beta_clip_min, beta_clip_max = 1.e-4, 1.e-2 # 0.999\n",
    "# epsilon = 8.e-3\n",
    "# steps = torch.arange(n_steps+1).to(torch.float32)\n",
    "# f_t = torch.cos(((steps/n_steps + epsilon) / (1.0 + epsilon))*torch.pi*0.5)**2\n",
    "# betas = torch.clip(1.0-f_t[1:]/f_t[:n_steps], beta_clip_min, beta_clip_max).tolist()\n",
    "\n",
    "# - cosine (https://arxiv.org/pdf/2102.09672.pdf)\n",
    "m = 0\n",
    "n = n_steps - m\n",
    "beta_clip_min, beta_clip_max = 0., 2.e-2 #1.e-4, 1.5e-2 #1.e-3, 1.e-2 #5.e-4, 1.2e-2 #1.e-4, 2.e-2 #5.e-4, 1.2e-2\n",
    "epsilon = 5.e-2\n",
    "steps = torch.arange(n+1).to(torch.float32)\n",
    "f_t = torch.cos(((steps/n + epsilon) / (1.0 + epsilon))*torch.pi*0.5)**2\n",
    "betas = torch.clip(1.0-f_t[1:]/f_t[:n], beta_clip_min, beta_clip_max).tolist()\n",
    "betas = betas + m*[betas[-1]]\n",
    "\n",
    "# Hyper parameters (design of the model)\n",
    "nf_list = (np.full(8, 25)*data_set.n_node_features).tolist()\n",
    "\n",
    "ddpm_net_hyper_params = dict(\n",
    "    n_node_features = data_set.n_node_features,\n",
    "    nf_list         = nf_list,\n",
    "    nf_last         = None, #8*data_set.n_node_features, #None,\n",
    "    has_mid         = True, #True, #False,\n",
    "    nf_mid          = nf_list[-2], #None,\n",
    "    activation      = torch.nn.LeakyReLU(), #torch.nn.ReLU(), #torch.nn.SiLU(),\n",
    "    te_activation   = torch.nn.LeakyReLU(), #torch.nn.ReLU(), #torch.nn.SiLU(),\n",
    "    normalize_down  = True,\n",
    "    op_down1        = 'SAGEConv', #'ResGatedGraphConv', #'GATConv', #'GraphConv', #'SAGEConv', #'GCNConv',\n",
    "    op_down2        = 'SAGEConv', #None, #'GraphConv',\n",
    "    normalize_up    = True,\n",
    "    op_up1          = 'SAGEConv',\n",
    "    op_up2          = 'SAGEConv', #None, #'GraphConv',\n",
    "    normalize_mid   = True,\n",
    "    op_mid1         = 'SAGEConv',\n",
    "    op_mid2         = 'SAGEConv', #None, #'GraphConv',\n",
    "    normalize_last  = False, #False,\n",
    "    op_last         = 'Linear', #'Linear', #'GraphConv',\n",
    "    n_steps         = n_steps, \n",
    "    time_emb_dim    = time_emb_dim\n",
    ")\n",
    "\n",
    "ddpm_hyper_params = dict(\n",
    "    n_node_features = data_set.n_node_features,\n",
    "    n_steps         = n_steps,\n",
    "    betas           = betas,\n",
    "    learn_noise     = True,\n",
    "    force_snr_zero  = False,\n",
    ")\n",
    "\n",
    "ddpm = Graph_DDPM(Graph_DDPM_net_model(**ddpm_net_hyper_params), **ddpm_hyper_params)\n",
    "\n",
    "# nb_net_params(ddpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model (hyper parameters and parameters) - if existing and already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print('Load the model (hyper parameters and parameters) (graphDDPM)...')\n",
    "\n",
    "# # Define activation function as they appear in hyper parameters...\n",
    "# from torch.nn import LeakyReLU, ReLU, SiLU, Sigmoid, Tanh\n",
    "\n",
    "# # Load model\n",
    "\n",
    "# # Hyper parameters (design of the model)\n",
    "# with open(filename_hyper_param_ddpm_net, 'r') as f: ddpm_net_hyper_params = eval(f.read())\n",
    "# with open(filename_hyper_param_ddpm, 'r') as f: ddpm_hyper_params = eval(f.read())\n",
    "\n",
    "# # # Hyper parameters (design of the model)\n",
    "# # with open(filename_hyper_param_ddpm_net, 'r') as f: ddpm_net_hyper_params = json.load(f)\n",
    "# # with open(filename_hyper_param_ddpm, 'r') as f: ddpm_hyper_params = json.load(f)\n",
    "\n",
    "# # Model (parameters)\n",
    "# ddpm = Graph_DDPM(Graph_DDPM_net_model(**ddpm_net_hyper_params), **ddpm_hyper_params)\n",
    "# ddpm.load_state_dict(torch.load(filename_param_ddpm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print('Load loss and lr...')\n",
    "\n",
    "# # Load loss and lr\n",
    "# with open(filename_loss_lr, 'rb') as f: train_loss, valid_loss, lr_used = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Display the model (graphDDPM)...')\n",
    "\n",
    "print(ddpm)\n",
    "print(f'Number of (learnable) params: {nb_net_params(ddpm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddpm.state_dict() # display parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some check on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise schedule and variance of features as function of time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Do some check (plot): noise schedule and variance of features as function of time step...')\n",
    "\n",
    "# Noise schedule and variance of features after each time step\n",
    "\n",
    "figsize = figsize_lh3\n",
    "\n",
    "plt.subplots(1,2, figsize=figsize)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ddpm.betas.to('cpu').numpy())\n",
    "#plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.title('Noise schedule (betas)')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(1.0 - ddpm.alpha_bars.to('cpu').numpy())\n",
    "plt.grid()\n",
    "plt.title(f'Variance (1 - alpha_bars, last = {1.0 - ddpm.alpha_bars[-1]:.5})')\n",
    "\n",
    "if save_fig_png:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_dir}/{fig_prefix}_{fig_counter:02d}_ddpm_schedule.png')\n",
    "    fig_counter = fig_counter+1\n",
    "\n",
    "if plt_show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features after all time steps VS normal N(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Do some check (plot): features after all time steps VS N(0,1)...')\n",
    "\n",
    "# Check that the noise (on graph node features), after all steps of forward process, follows a normal distribution N(0, 1)\n",
    "\n",
    "d_set = train_set\n",
    "#d_set = data_set\n",
    "\n",
    "# Get features\n",
    "# ------------\n",
    "# n = len(set)\n",
    "# batch = next(iter(torch_geometric.loader.DataLoader(d_set, batch_size=n, shuffle=False)))\n",
    "\n",
    "torch.random.manual_seed(8764) # -> for reproducibility (if needed)\n",
    "\n",
    "batch_size = 50\n",
    "data_loader = torch_geometric.loader.DataLoader(d_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = 'cuda:0'\n",
    "ddpm.to_device(device)\n",
    "Z = []\n",
    "for G_batch in data_loader:\n",
    "    G_batch = ddpm.forward(G_batch.to(device), torch.full((G_batch.num_graphs, ), ddpm.n_steps-1, device=device))\n",
    "    Z.append(G_batch.x.to('cpu').numpy())\n",
    "ddpm.to_device('cpu')\n",
    "\n",
    "Z = np.vstack(Z) # one features per column\n",
    "\n",
    "# Kullback-Leibler divergence of N(0, 1) and distribution of graph node feature (Zi)\n",
    "# ----------------------------------------------------------------------------------\n",
    "u = np.linspace(-4.0, 4.0, 1000)\n",
    "# For the intervals:\n",
    "#   [-inf, u[0]], [u[i], u[i+1]], i=0, ..., len(u)-1, [u[-1], inf]\n",
    "# compute\n",
    "#    Nu: probability that N(0,1) distribution is in each of these intervals\n",
    "#    Zu: probability that distribution of Z[:, i] is in each of these intervals\n",
    "cdf_Nu = scipy.stats.norm.cdf(u)\n",
    "Nu = np.insert(np.append(np.diff(cdf_Nu), 1.0-cdf_Nu[-1]), 0, cdf_Nu[0])\n",
    "\n",
    "kld_N01_Z = np.zeros(Z.shape[1])\n",
    "for i in range(Z.shape[1]):\n",
    "    # Compute Zu\n",
    "    Z_kde = scipy.stats.gaussian_kde(Z[:, i])\n",
    "    Zu = [Z_kde.integrate_box(u[i], u[i+1]) for i in range(len(u)-1)]\n",
    "    Zu = np.hstack(([Z_kde.integrate_box(-np.inf, u[0])], Zu))\n",
    "    Zu = np.hstack((Zu, [1.0 - Zu.sum()]))\n",
    "    \n",
    "    # Compute Kullback-Leibler divergence KL(Nu, Zu)\n",
    "    kld_N01_Z[i] = scipy.special.rel_entr(Nu, Zu).sum()\n",
    "\n",
    "# QQ-plot of distribution of graph node feature (Zi) vs N(0, 1)\n",
    "# -------------------------------------------------------------\n",
    "q = np.linspace(.001, .999, 100)\n",
    "Zq = np.quantile(Z, q=q, axis=0) # Zq[i, j] : q[i]-quantile of Z[:, j]\n",
    "Nq = scipy.stats.norm.ppf(q)\n",
    "\n",
    "# Plots\n",
    "# -----\n",
    "nr = Zq.shape[1]\n",
    "nc = 2\n",
    "\n",
    "figsize = (figsize_lh3[0], figsize_lh3[1]*nr*.75)\n",
    "\n",
    "plt.subplots(nr, nc, figsize=figsize)\n",
    "\n",
    "for j in range(nr):\n",
    "    plt.subplot(nr, nc, j*nc+1)\n",
    "    plt.plot(Zq[:, j], Nq, ls='', marker='.')\n",
    "    mi, ma = Zq[:, j].min(), Zq[:, j].max()\n",
    "    plt.plot([mi, ma], [mi, ma], ls='dashed')\n",
    "    plt.grid()\n",
    "    plt.xlabel(f'Z{j}')\n",
    "    plt.ylabel('N(0,1)')\n",
    "    plt.text(mi, ma, f'KLdiv(N(0,1) || Z{j})={kld_N01_Z[j]:.4g}', \n",
    "               fontsize=12, va='top', ha='left',\n",
    "               bbox={'facecolor':'white', 'alpha':.8, 'edgecolor':'black', 'boxstyle':'round, pad=0.2'})\n",
    "    if j==0:\n",
    "        plt.title('QQ-plot')\n",
    "\n",
    "    plt.subplot(nr, nc, j*nc+2)\n",
    "    plt.hist(Z[:, j], bins=50, density=True, label=f'Z{j}')\n",
    "    t = np.linspace(Z[:, j].min(), Z[:, j].max(), 300)\n",
    "    plt.plot(t, scipy.stats.norm.pdf(t), ls='dashed', label='N(0,1)')\n",
    "    plt.axvline(x=np.mean(Z[:, j]), ls='dashed', color='purple', label='mean')\n",
    "    plt.grid()\n",
    "    plt.xlabel(f'Z{j}')\n",
    "    plt.legend(fontsize=8)\n",
    "    if j==0:\n",
    "        plt.title('Density')\n",
    "\n",
    "if save_fig_png:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_dir}/{fig_prefix}_{fig_counter:02d}_ddpm_check_noise.png')\n",
    "    fig_counter = fig_counter+1\n",
    "\n",
    "if plt_show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all features (together): Z_all\n",
    "# ----------------------------------\n",
    "Z_all = Z.reshape(-1)\n",
    "\n",
    "# Kullback-Leibler divergence of N(0, 1) and distribution of all graph node features (Z_all)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "u = np.linspace(-4.0, 4.0, 1000)\n",
    "# For the intervals:\n",
    "#   [-inf, u[0]], [u[i], u[i+1]], i=0, ..., len(u)-1, [u[-1], inf]\n",
    "# compute\n",
    "#    Nu: probability that N(0,1) distribution is in each of these intervals\n",
    "#    Zu: probability that distribution of Z_all is in each of these intervals\n",
    "cdf_Nu = scipy.stats.norm.cdf(u)\n",
    "Nu = np.insert(np.append(np.diff(cdf_Nu), 1.0-cdf_Nu[-1]), 0, cdf_Nu[0])\n",
    "\n",
    "# Compute Zu\n",
    "Z_kde = scipy.stats.gaussian_kde(Z_all)\n",
    "Zu = [Z_kde.integrate_box(u[i], u[i+1]) for i in range(len(u)-1)]\n",
    "Zu = np.hstack(([Z_kde.integrate_box(-np.inf, u[0])], Zu))\n",
    "Zu = np.hstack((Zu, [1.0 - Zu.sum()]))\n",
    "    \n",
    "# Compute Kullback-Leibler divergence KL(Nu, Zu)\n",
    "kld_N01_Z_all = scipy.special.rel_entr(Nu, Zu).sum()\n",
    "\n",
    "# QQ-plot of distribution of all graph node features (Z_all) vs N(0, 1)\n",
    "# ---------------------------------------------------------------------\n",
    "#q = np.linspace(.001, .999, 100)\n",
    "Zq = np.quantile(Z_all, q=q)\n",
    "Nq = scipy.stats.norm.ppf(q)\n",
    "\n",
    "# Plots\n",
    "# -----\n",
    "figsize = figsize_lh3\n",
    "\n",
    "plt.subplots(1,2,figsize=figsize)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(Zq, Nq, ls='', marker='.')\n",
    "plt.plot([Zq.min(), Zq.max()], [Zq.min(), Zq.max()], ls='dashed')\n",
    "plt.grid()\n",
    "plt.xlabel('Z (all)')\n",
    "plt.ylabel('N(0,1)')\n",
    "plt.text(mi, ma, f'KLdiv(N(0,1) || Z_all)={kld_N01_Z_all:.4g}', \n",
    "            fontsize=12, va='top', ha='left',\n",
    "            bbox={'facecolor':'white', 'alpha':.8, 'edgecolor':'black', 'boxstyle':'round, pad=0.2'})\n",
    "plt.title('QQ-plot')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(Zq, bins=50, density=True, label='Z')\n",
    "t = np.linspace(Z_all.min(), Z_all.max(), 300)\n",
    "plt.plot(t, scipy.stats.norm.pdf(t), ls='dashed', label='N(0,1)')\n",
    "plt.axvline(x=np.mean(Z_all), ls='dashed', color='purple', label='mean')\n",
    "plt.grid()\n",
    "plt.xlabel('Z (all)')\n",
    "plt.legend()\n",
    "plt.title('Density')\n",
    "\n",
    "if save_fig_png:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_dir}/{fig_prefix}_{fig_counter:02d}_ddpm_check_noise_all_features_grouped.png')\n",
    "    fig_counter = fig_counter+1\n",
    "\n",
    "if plt_show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: diffusion of one graph is illustrated in next notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = (ddpm.betas * (1.0 - ddpm.alpha_bars_prev) / (1.0 - ddpm.alpha_bars)).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Set loss function...')\n",
    "\n",
    "# Loss function\n",
    "# -------------\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-initialize the model parameters (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[Re-]initialize the model parameters...')\n",
    "\n",
    "# Initialize the network parameters\n",
    "# ---------------------------------\n",
    "ddpm_seed = 903\n",
    "torch.random.manual_seed(ddpm_seed)\n",
    "#ddpm.net.init_weights(gain=0.1)\n",
    "reset_all_parameters(ddpm)\n",
    "    \n",
    "# Initialize lists for storing loss, lr\n",
    "# -------------------------------------\n",
    "train_loss, valid_loss = [], []\n",
    "lr_used = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train the model...')\n",
    "\n",
    "# Re-launch as many times as needed (and change the settings below if needed)!\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Settings\n",
    "# ========\n",
    "\n",
    "# Batch size, number of epochs\n",
    "# ----------------------------\n",
    "batch_size = 50\n",
    "num_epochs = 1201\n",
    "print_epoch = 1\n",
    "\n",
    "# Optimizer\n",
    "# ---------------------\n",
    "# # - 1. Stochastic Gradient Descent (SGD)\n",
    "# lr = .001             # learning rate\n",
    "# weight_decay = 0.00 # L2-regularization\n",
    "# momentum = 0.00     # momentum\n",
    "# optimizer = torch.optim.SGD(ddpm.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "# - 2. Adam\n",
    "lr = 0.001            # learning rate; default: lr=0.001\n",
    "adam_betas = (0.9, 0.999)    # betas parameters; default: betas=(0.9, 0.999)\n",
    "# lr = 0.0002           # learning rate; default: lr=0.001\n",
    "# adam_betas = (0.5, 0.999)    # betas parameters; default: betas=(0.9, 0.999)\n",
    "eps = 1.e-8           # epsilon parameter; default: 1.e-8\n",
    "weight_decay = 0.000   # L2-regularization; default: 0.0\n",
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=lr, betas=adam_betas, eps=eps, weight_decay=weight_decay)\n",
    "# ...\n",
    "\n",
    "# Learning rate scheduler\n",
    "# -----------------------\n",
    "# # - 1. CosineAnnealingLR\n",
    "# lr_init = lr\n",
    "# T_max = num_epochs / 7. # num_epochs\n",
    "# eta_min = lr_init / 5.\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min, last_epoch=-1, verbose=False)\n",
    "# # Essentially, with eta_max = lr_init, at epoch t, the learning rate is set to\n",
    "# #    eta_t = eta_min + 1/2 * (eta_max - eta_min) * (1 + cos(t/T_max*pi))\n",
    "#\n",
    "# - 2. CosineAnnealingWarmRestarts\n",
    "lr_init = lr\n",
    "eta_min = lr_init / 5.\n",
    "T_mult = 2\n",
    "nrestart = 3\n",
    "T_0 = int(np.ceil(num_epochs * (T_mult-1) / (T_mult**(nrestart+1)-1)))\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min, last_epoch=-1, verbose=False)\n",
    "# Essentially, with eta_max = lr_init, at epoch t, the learning rate is set to\n",
    "#    eta_t = eta_min + 1/2 * (eta_max - eta_min) * (1 + cos(T_cur/T_i*pi))\n",
    "# where\n",
    "#    T_cur: the number of epochs since the last restart\n",
    "#    T_i  : the number of epochs between two warm restarts\n",
    "#    T_0  : number of epochs before the 1st warm restart\n",
    "#    T_mult: T_i is defined as T_i = T_mult * T_{i-1}\n",
    "#\n",
    "# # - 3. MultiStepLR\n",
    "# gamma = .3\n",
    "# milestones = [int(num_epochs/9), int(num_epochs/3)]\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=gamma, last_epoch=-1, verbose=False)\n",
    "# # At epoch in `milestones`, lr is multiplied by `gamma`\n",
    "#\n",
    "# # - 4.\n",
    "# lr_scheduler = None\n",
    "# ...\n",
    "\n",
    "# ----\n",
    "# # Note: to compute the sequence of lr used, and plot it: \n",
    "# lr_used = []\n",
    "# for i in range(num_epochs):\n",
    "#     lr_used.append(lr_scheduler.get_last_lr()[0])\n",
    "#     optimizer.step()\n",
    "#     lr_scheduler.step()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(lr_used)\n",
    "# plt.xlabel('epoch')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "# ========\n",
    "\n",
    "# ---\n",
    "# Fixed batch of graph with random noise for generating data to save during training\n",
    "ng_fixed = 12 # number of graphs\n",
    "torch.manual_seed(983)\n",
    "\n",
    "# # Set G_batch_fixed\n",
    "# G_batch_1 = next(iter(torch_geometric.loader.DataLoader(train_set, batch_size=ng_fixed//2, shuffle=True)))\n",
    "# G_batch_2 = next(iter(torch_geometric.loader.DataLoader(valid_set, batch_size=ng_fixed - ng_fixed//2, shuffle=True)))\n",
    "# G_batch_fixed = torch_geometric.data.Batch.from_data_list(G_batch_1.to_data_list() + G_batch_2.to_data_list())\n",
    "\n",
    "# Set G_batch_fixed\n",
    "G_batch_fixed_initial = next(iter(torch_geometric.loader.DataLoader(test_set, batch_size=12, shuffle=True)))\n",
    "G_batch_fixed = G_batch_fixed_initial.clone()\n",
    "\n",
    "# # Set G_batch_fixed\n",
    "# G_batch_fixed = next(iter(torch_geometric.loader.DataLoader(data_set, batch_size=12, shuffle=True)))\n",
    "\n",
    "torch.manual_seed(878)\n",
    "G_batch_fixed.x = torch.randn_like(G_batch_fixed.x)\n",
    "save_gen_epoch = 200 # save generated data at every `save_epoch`\n",
    "save_gen_dir = out_dir + '/train_intermediate'\n",
    "save_gen_file_fmt= save_gen_dir + '/ddpm_{:04d}.pt' # where to save generated data\n",
    "if not os.path.isdir(save_gen_dir):\n",
    "    os.mkdir(save_gen_dir)\n",
    "# ---\n",
    "\n",
    "# Create Data Loader for training data set\n",
    "train_data_loader = torch_geometric.loader.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create Data Loader for validation data set\n",
    "valid_batch_size = 100\n",
    "valid_data_loader = torch_geometric.loader.DataLoader(valid_set, batch_size=valid_batch_size, shuffle=False)\n",
    "# valid_data_loader = None\n",
    "\n",
    "# Train\n",
    "t1 = time.time()\n",
    "train_loss_cur, valid_loss_cur, lr_used_cur = \\\n",
    "    train_graph_ddpm(\n",
    "        train_data_loader, \n",
    "        ddpm, \n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        return_lr=True,\n",
    "        return_loss=True,\n",
    "        num_epochs=num_epochs,\n",
    "        valid_data_loader=valid_data_loader,\n",
    "        print_epoch=print_epoch,\n",
    "        G_batch_fixed=G_batch_fixed,\n",
    "        save_gen_epoch=save_gen_epoch,\n",
    "        save_gen_file_fmt=save_gen_file_fmt,\n",
    "        # device=torch.device('cpu')\n",
    "        device=torch.device('cuda:0')\n",
    "    )\n",
    "t2 = time.time()\n",
    "\n",
    "# Update lists of loss, accuracy, score, lr\n",
    "train_loss = train_loss + train_loss_cur # concatenate list\n",
    "valid_loss = valid_loss + valid_loss_cur # concatenate list\n",
    "\n",
    "lr_used = lr_used + lr_used_cur # concatenate list\n",
    "\n",
    "# Print elapsed time and result of last epoch\n",
    "print(f'Elapsed time for {num_epochs} epochs: {t2-t1:.3g} s')\n",
    "print(f'Last epoch, loss : train: {train_loss[-1]:.2g}, valid: {valid_loss[-1]:.2g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot loss...')\n",
    "\n",
    "# Plot loss (training and validation)\n",
    "# -----------------------------------\n",
    "color_train = 'tab:blue'\n",
    "color_valid = 'tab:red'\n",
    "\n",
    "figsize = figsize_lh3\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "# loss\n",
    "plt.plot(train_loss, ls='-', color=color_train, label='train loss')\n",
    "plt.plot(valid_loss, ls='-', color=color_valid, label='valid loss')\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('loss')\n",
    "\n",
    "if save_fig_png:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_dir}/{fig_prefix}_{fig_counter:02d}_ddpm_loss.png')\n",
    "    fig_counter = fig_counter+1\n",
    "\n",
    "if plt_show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot loss (log scale)...')\n",
    "\n",
    "# Plot loss (training and validation) (log scale along y axis)\n",
    "# -----------------------------------\n",
    "color_train = 'tab:blue'\n",
    "color_valid = 'tab:red'\n",
    "\n",
    "figsize = figsize_lh3\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "# loss\n",
    "plt.plot(train_loss, ls='-', color=color_train, label='train loss')\n",
    "plt.plot(valid_loss, ls='-', color=color_valid, label='valid loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('loss')\n",
    "\n",
    "if save_fig_png:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_dir}/{fig_prefix}_{fig_counter:02d}_ddpm_loss_logscale.png')\n",
    "    fig_counter = fig_counter+1\n",
    "\n",
    "if plt_show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot loss (part) (log scale)...')\n",
    "\n",
    "nskip_beg = int(0.3*num_epochs)\n",
    "nskip_end = 0\n",
    "\n",
    "figsize = figsize_lh3\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "# loss\n",
    "plt.plot(np.arange(nskip_beg, num_epochs-nskip_end), train_loss[nskip_beg:num_epochs-nskip_end], ls='-', color=color_train, label='train loss')\n",
    "plt.plot(np.arange(nskip_beg, num_epochs-nskip_end), valid_loss[nskip_beg:num_epochs-nskip_end], ls='-', color=color_valid, label='valid loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('loss')\n",
    "\n",
    "if save_fig_png:\n",
    "    plt.tight_layout()\n",
    "    fig_counter = fig_counter-1\n",
    "    plt.savefig(f'{fig_dir}/{fig_prefix}_{fig_counter:02d}_ddpm_loss_logscale_zoom1.png')\n",
    "    fig_counter = fig_counter+1\n",
    "\n",
    "if plt_show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot lr...')\n",
    "\n",
    "# Plot learning rate (lr)\n",
    "# -----------------------\n",
    "color_lr = 'orange'\n",
    "\n",
    "figsize = figsize_lh3\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(lr_used, ls='-', color=color_lr, label='lr ddpm')\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('lr')\n",
    "\n",
    "if save_fig_png:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_dir}/{fig_prefix}_{fig_counter:02d}_ddpm_lr.png')\n",
    "    fig_counter = fig_counter+1\n",
    "\n",
    "if plt_show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show data generated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots\n",
    "mr = 1                  # nb of line\n",
    "mc = min(8, ng_fixed)   # nb of cols (graph in one line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot data generated during training (2D)...')\n",
    "\n",
    "# 2D view \n",
    "# =======\n",
    "kwds = kwds_multi_line.copy()\n",
    "\n",
    "figsize = figsize_multi_line\n",
    "# -----\n",
    "\n",
    "# Plot generated data at given epoch, saved during training\n",
    "# ---------------------------------------------------------\n",
    "epoch_list = list(range(0, num_epochs, save_gen_epoch))\n",
    "\n",
    "for epoch in epoch_list:\n",
    "    G_batch = torch.load(save_gen_file_fmt.format(epoch))\n",
    "    G_batch_geom_list = G_batch.to_data_list()[:mc]\n",
    "    out_name = f'ddpm_train_2d_after_epoch_{epoch}'\n",
    "\n",
    "    plot_graph_multi_2d_from_G_geom_list(\n",
    "            G_batch_geom_list, dim,\n",
    "            out_name=out_name, \n",
    "            nr=1,\n",
    "            attr=attr,\n",
    "            attr_label_list=attr_label_list, \n",
    "            attr_cmap_list=attr_cmap_list,\n",
    "            rescale=False,\n",
    "            title_list=None, title_fontsize=8,\n",
    "            figsize=figsize, save_fig_png=save_fig_png, \n",
    "            filename_prefix=f'{fig_dir}/{fig_prefix}_{fig_counter:02d}',\n",
    "            with_labels=False, same_color_bar=True, show_color_bar=True,\n",
    "            show=plt_show,\n",
    "            **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with graph of fixed batch\n",
    "# ------------------------------------\n",
    "G_batch_geom_list = G_batch_fixed_initial.to_data_list()[:mc]\n",
    "out_name = f'ddpm_train_2d_initial_graphs'\n",
    "\n",
    "plot_graph_multi_2d_from_G_geom_list(\n",
    "        G_batch_geom_list, dim,\n",
    "        out_name=out_name, \n",
    "        nr=1,\n",
    "        attr=attr,\n",
    "        attr_label_list=attr_label_list, \n",
    "        attr_cmap_list=attr_cmap_list,\n",
    "        rescale=False,\n",
    "        title_list=None, title_fontsize=8,\n",
    "        figsize=figsize, save_fig_png=save_fig_png, \n",
    "        filename_prefix=f'{fig_dir}/{fig_prefix}_{fig_counter:02d}',\n",
    "        with_labels=False, same_color_bar=True, show_color_bar=True,\n",
    "        show=plt_show,\n",
    "        **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%skip_if dim == 2\n",
    "if dim == 3:\n",
    "    print('Plot data generated during training (3D)...')\n",
    "\n",
    "    # 3D view \n",
    "    # =======\n",
    "    kwargs_edges = kwargs_edges_multi_line.copy()\n",
    "    kwargs_pts = kwargs_pts_multi_line.copy()\n",
    "    kwargs_pts_labels = kwargs_pts_labels_multi_line.copy()\n",
    "    kwargs_scalar_bar = kwargs_scalar_bar_multi_line.copy()\n",
    "\n",
    "    window_size = window_size_multi_line\n",
    "    # -----\n",
    "\n",
    "    notebook = True  # inline\n",
    "    cpos = None\n",
    "\n",
    "    # Plot generated data at given epoch, saved during training\n",
    "    # ---------------------------------------------------------\n",
    "    epoch_list = list(range(0, num_epochs, save_gen_epoch))\n",
    "\n",
    "    for epoch in epoch_list:\n",
    "        G_batch = torch.load(save_gen_file_fmt.format(epoch))\n",
    "        G_batch_geom_list = G_batch.to_data_list()[:mc]\n",
    "        out_name = f'ddpm_train_3d_after_epoch_{epoch}'\n",
    "\n",
    "        plot_graph_multi_3d_from_G_geom_list(\n",
    "                G_batch_geom_list, dim,\n",
    "                out_name=out_name, \n",
    "                nr=1,\n",
    "                attr=attr,\n",
    "                attr_label_list=attr_label_list, \n",
    "                attr_cmap_list=attr_cmap_list,\n",
    "                rescale=False,\n",
    "                title_list=None, title_fontsize=8,\n",
    "                notebook=notebook, window_size=window_size, save_fig_png=save_fig_png, off_screen=off_screen,\n",
    "                filename_prefix=f'{fig_dir}/{fig_prefix}_{fig_counter:02d}',\n",
    "                with_labels=False, same_color_bar=True, show_color_bar=True,\n",
    "                kwargs_edges=kwargs_edges, kwargs_pts=kwargs_pts, kwargs_scalar_bar=kwargs_scalar_bar, kwargs_pts_labels=kwargs_pts_labels,\n",
    "                cpos=cpos, print_cpos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%skip_if dim == 2\n",
    "if dim == 3:\n",
    "\n",
    "    notebook = True  # inline\n",
    "    cpos = None\n",
    "\n",
    "    # Comparison with graph of fixed batch\n",
    "    # ------------------------------------\n",
    "    G_batch_geom_list = G_batch_fixed_initial.to_data_list()[:mc]\n",
    "    out_name = f'ddpm_train_3d_initial_graphs'\n",
    "\n",
    "    plot_graph_multi_3d_from_G_geom_list(\n",
    "            G_batch_geom_list, dim,\n",
    "            out_name=out_name, \n",
    "            nr=1,\n",
    "            attr=attr,\n",
    "            attr_label_list=attr_label_list, \n",
    "            attr_cmap_list=attr_cmap_list,\n",
    "            rescale=False,\n",
    "            title_list=None, title_fontsize=8,\n",
    "            notebook=notebook, window_size=window_size, save_fig_png=save_fig_png, off_screen=off_screen,\n",
    "            filename_prefix=f'{fig_dir}/{fig_prefix}_{fig_counter:02d}',\n",
    "            with_labels=False, same_color_bar=True, show_color_bar=True,\n",
    "            kwargs_edges=kwargs_edges, kwargs_pts=kwargs_pts, kwargs_scalar_bar=kwargs_scalar_bar, kwargs_pts_labels=kwargs_pts_labels,\n",
    "            cpos=cpos, print_cpos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig_png:\n",
    "    fig_counter = fig_counter+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data set / test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save / export the data set / test set (graphDDPM)...')\n",
    "\n",
    "# Save data set\n",
    "with open(filename_data_set, 'wb') as f: pickle.dump(data_set, file=f)\n",
    "with open(filename_data_set_shift, 'w')  as f: np.savetxt(f, node_features_shift)\n",
    "with open(filename_data_set_scale_factor, 'w')  as f: np.savetxt(f, node_features_scale_factor)\n",
    "\n",
    "# Save test set\n",
    "with open(filename_test_set, 'wb') as f: pickle.dump(test_set, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model (hyper parameters and parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save / export the model (hyper parameters and parameters)...')\n",
    "\n",
    "# Save model\n",
    "\n",
    "# # Hyper parameters (design of the model)\n",
    "# with open(filename_hyper_param_ddpm_net, 'w') as f: json.dump(ddpm_net_hyper_params, f)\n",
    "# with open(filename_hyper_param_ddpm, 'w')     as f: json.dump(ddpm_hyper_params, f)\n",
    "\n",
    "# Hyper parameters (design of the model)\n",
    "with open(filename_hyper_param_ddpm_net, 'w') as f: f.write(str(ddpm_net_hyper_params).replace(',', ',\\n'))\n",
    "with open(filename_hyper_param_ddpm, 'w')     as f: f.write(str(ddpm_hyper_params).replace(',', ',\\n'))\n",
    "\n",
    "# Model parameters\n",
    "torch.save(ddpm.state_dict(), filename_param_ddpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save / export loss and lr...')\n",
    "\n",
    "# Save loss and lr\n",
    "with open(filename_loss_lr, 'wb') as f: pickle.dump((train_loss, valid_loss, lr_used), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Display the model (graphDDPM)...')\n",
    "\n",
    "print(ddpm)\n",
    "print(f'Number of (learnable) params: {nb_net_params(ddpm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddpm.state_dict() # display parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
